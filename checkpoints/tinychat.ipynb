{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# tinyChat","metadata":{}},{"cell_type":"markdown","source":"### A RAG integrated Chatbot, powered by TinyLlama 1.1B as main LM\nBuilt By: [Mohammad Ali](https://github.com/mohammad17ali)","metadata":{}},{"cell_type":"markdown","source":"## 1. Checking and Importing Requirements","metadata":{}},{"cell_type":"code","source":"pip install llama-index transformers torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T09:08:39.351606Z","iopub.execute_input":"2025-04-01T09:08:39.351896Z","iopub.status.idle":"2025-04-01T09:08:50.243667Z","shell.execute_reply.started":"2025-04-01T09:08:39.351875Z","shell.execute_reply":"2025-04-01T09:08:50.242747Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting llama-index\n  Downloading llama_index-0.12.27-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\nCollecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting llama-index-core<0.13.0,>=0.12.27 (from llama-index)\n  Downloading llama_index_core-0.12.27-py3-none-any.whl.metadata (2.6 kB)\nCollecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\nCollecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n  Downloading llama_index_indices_managed_llama_cloud-0.6.10-py3-none-any.whl.metadata (3.6 kB)\nCollecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n  Downloading llama_index_llms_openai-0.3.29-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\nCollecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\nCollecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\nCollecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\nCollecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\nCollecting nltk>3.8.1 (from llama-index)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.57.4)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.27->llama-index) (2.0.36)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (3.11.12)\nCollecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.27->llama-index)\n  Downloading banks-2.1.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (1.2.15)\nCollecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.27->llama-index)\n  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\nCollecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.27->llama-index)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (0.28.1)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (1.6.0)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (11.0.0)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (2.11.0a2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (9.0.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (0.9.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (0.9.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.27->llama-index) (1.17.0)\nCollecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n  Downloading llama_cloud-0.1.17-py3-none-any.whl.metadata (902 bytes)\nCollecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n  Downloading openai-1.70.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.3)\nRequirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\nCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.27->llama-index) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.27->llama-index) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.27->llama-index) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.27->llama-index) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.27->llama-index) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.27->llama-index) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.27->llama-index) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.27->llama-index) (1.18.3)\nCollecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.27->llama-index)\n  Downloading griffe-1.7.1-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.27->llama-index) (4.3.6)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.27->llama-index) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.27->llama-index) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.27->llama-index) (0.14.0)\nCollecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading llama_cloud_services-0.6.9-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.27->llama-index) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.27->llama-index) (2.29.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.27->llama-index) (3.1.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.27->llama-index) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.27->llama-index) (3.26.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.27->llama-index) (1.2.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nCollecting platformdirs (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.27->llama-index)\n  Downloading platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)\nCollecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\nRequirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.10/dist-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.27->llama-index) (0.4.6)\nDownloading llama_index-0.12.27-py3-none-any.whl (7.0 kB)\nDownloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\nDownloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\nDownloading llama_index_core-0.12.27-py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\nDownloading llama_index_indices_managed_llama_cloud-0.6.10-py3-none-any.whl (14 kB)\nDownloading llama_index_llms_openai-0.3.29-py3-none-any.whl (23 kB)\nDownloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\nDownloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\nDownloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\nDownloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading banks-2.1.1-py3-none-any.whl (28 kB)\nDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\nDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading llama_cloud-0.1.17-py3-none-any.whl (253 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\nDownloading openai-1.70.0-py3-none-any.whl (599 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\nDownloading llama_cloud_services-0.6.9-py3-none-any.whl (29 kB)\nDownloading platformdirs-4.3.7-py3-none-any.whl (18 kB)\nDownloading griffe-1.7.1-py3-none-any.whl (129 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: striprtf, filetype, dirtyjson, python-dotenv, platformdirs, nltk, griffe, openai, llama-cloud, banks, llama-index-core, llama-index-llms-openai, llama-index-agent-openai, llama-cloud-services, llama-parse, llama-index-program-openai, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-readers-file, llama-index-question-gen-openai, llama-index-multi-modal-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-cli, llama-index\n  Attempting uninstall: platformdirs\n    Found existing installation: platformdirs 4.3.6\n    Uninstalling platformdirs-4.3.6:\n      Successfully uninstalled platformdirs-4.3.6\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: openai\n    Found existing installation: openai 1.57.4\n    Uninstalling openai-1.57.4:\n      Successfully uninstalled openai-1.57.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed banks-2.1.1 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.1 llama-cloud-0.1.17 llama-cloud-services-0.6.9 llama-index-0.12.27 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.27 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.10 llama-index-llms-openai-0.3.29 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 nltk-3.9.1 openai-1.70.0 platformdirs-4.3.7 python-dotenv-1.1.0 striprtf-0.0.26\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T09:08:50.244859Z","iopub.execute_input":"2025-04-01T09:08:50.245186Z","iopub.status.idle":"2025-04-01T09:08:56.604756Z","shell.execute_reply.started":"2025-04-01T09:08:50.245148Z","shell.execute_reply":"2025-04-01T09:08:56.603586Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport transformers\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling, pipeline)\nimport faiss\nfrom typing import List, Dict, Tuple\nimport textwrap\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T09:34:09.477137Z","iopub.execute_input":"2025-04-01T09:34:09.477475Z","iopub.status.idle":"2025-04-01T09:34:09.481970Z","shell.execute_reply.started":"2025-04-01T09:34:09.477448Z","shell.execute_reply":"2025-04-01T09:34:09.480994Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T09:34:18.507851Z","iopub.execute_input":"2025-04-01T09:34:18.508136Z","iopub.status.idle":"2025-04-01T09:34:28.308079Z","shell.execute_reply.started":"2025-04-01T09:34:18.508114Z","shell.execute_reply":"2025-04-01T09:34:28.307392Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89eac04ca815460e821fd22e0b03cef4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1746bcf37ec441d98a30cf443691a919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95ed3d7b218747b9bc749e677e23e3ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11883b78eddd4e1f9369c0f61618520e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0056d6fa7334b53be04130addeeb4d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de4b1a70211545d48c3bcc07614aa6f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5276a0619ba64997b178322d0d6752ec"}},"metadata":{}}],"execution_count":56},{"cell_type":"markdown","source":"#### Checking simple context passing and generation with TinyLlama, using fictional data which the model surely has never seen before.","metadata":{}},{"cell_type":"code","source":"context = ['Kvaratskhelia is a good midfielder.','Kadambaragu brother is Kvaratskhelia.', 'Kvaratskhelia plays for FC Barcelona.']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T09:34:30.538024Z","iopub.execute_input":"2025-04-01T09:34:30.538380Z","iopub.status.idle":"2025-04-01T09:34:30.542357Z","shell.execute_reply.started":"2025-04-01T09:34:30.538350Z","shell.execute_reply":"2025-04-01T09:34:30.541431Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": f\"The following is relevant context for your response. Use this information to help answer the user's question:\\n\\n{context}\"\n    },\n    {\"role\": \"user\", \"content\": \"What does Kadambaragu's brother do?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T09:34:32.823622Z","iopub.execute_input":"2025-04-01T09:34:32.823943Z","iopub.status.idle":"2025-04-01T09:34:36.819023Z","shell.execute_reply.started":"2025-04-01T09:34:32.823914Z","shell.execute_reply":"2025-04-01T09:34:36.818240Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"<|system|>\nThe following is relevant context for your response. Use this information to help answer the user's question:\n\n['Kvaratskhelia is a good midfielder.', 'Kadambaragu brother is Kvaratskhelia.', 'Kvaratskhelia plays for FC Barcelona.']</s>\n<|user|>\nWhat does Kadambaragu's brother do?</s>\n<|assistant|>\nKadambaragu's brother is Kvaratskhelia. Kvaratskhelia is not mentioned in the given context, so we do not know what his role or responsibilities are as a player for FC Barcelona.\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"## 2. Building the Bot","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n        self.embedding_model = 'all-MiniLM-L6-v2'\n        self.max_context_length = 512\n        self.max_new_tokens = 256\n        self.vector_dim = 384\n        self.top_k = 3\n        self.chunk_size = 256\n        self.stop_word = 'STOP!'\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:28:28.027900Z","iopub.execute_input":"2025-04-01T10:28:28.028243Z","iopub.status.idle":"2025-04-01T10:28:28.032711Z","shell.execute_reply.started":"2025-04-01T10:28:28.028214Z","shell.execute_reply":"2025-04-01T10:28:28.031875Z"}},"outputs":[],"execution_count":140},{"cell_type":"markdown","source":"### 2.2 Vector Database and Functionalities","metadata":{}},{"cell_type":"code","source":"class VectorDB:\n    def __init__(self, vector_dim: int):\n        self.vector_dim = vector_dim\n        self.index = faiss.IndexFlatL2(vector_dim)\n        self.texts = []\n        self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n    \n    def add_text(self, text: str) -> None:\n        chunks = self._create_chunks(text)\n        for chunk in chunks:\n            self._add_chunk(chunk)\n    \n    def _create_chunks(self, text: str) -> List[str]:\n        words = text.split()\n        chunks = []\n        current_chunk = []\n        current_length = 0\n        \n        for word in words:\n            current_chunk.append(word)\n            current_length += len(word) + 1 \n            \n            if current_length >= config.chunk_size:\n                chunks.append(' '.join(current_chunk))\n                current_chunk = []\n                current_length = 0\n                \n        if current_chunk:\n            chunks.append(' '.join(current_chunk))\n        return chunks\n\n    def _add_chunk(self, chunk: str) -> None:\n        embedding = self.embedding_model.encode([chunk])[0]\n        embedding = np.array([embedding], dtype=np.float32)\n        faiss.normalize_L2(embedding)  # Normalize before adding\n        self.index.add(embedding)\n        self.texts.append(chunk)\n    \n    def _add_chunk(self, chunk: str) -> None:\n        embedding = self.embedding_model.encode([chunk])[0]\n        faiss.normalize_L2(np.array([embedding], dtype=np.float32))\n        self.index.add(np.array([embedding], dtype=np.float32))\n        self.texts.append(chunk)\n\n    def DB(self, limit: int = 1) -> List[str]:\n        if self.index.ntotal == 0:\n            return ['empty DataBase']\n        \n        return self.texts[:min(limit, len(self.texts))]\n    \n    def search(self, query: str, top_k: int = 3) -> List[str]:\n        query_embedding = self.embedding_model.encode([query])[0]\n        query_embedding = np.array([query_embedding], dtype=np.float32)\n        faiss.normalize_L2(query_embedding)  # Normalize before searching\n        \n        if self.index.ntotal == 0:\n            return []\n        \n        D, I = self.index.search(query_embedding, min(top_k, self.index.ntotal))\n        \n        results = [self.texts[i] for i in I[0] if i < len(self.texts)]\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:28:28.691832Z","iopub.execute_input":"2025-04-01T10:28:28.692191Z","iopub.status.idle":"2025-04-01T10:28:28.701885Z","shell.execute_reply.started":"2025-04-01T10:28:28.692137Z","shell.execute_reply":"2025-04-01T10:28:28.701121Z"}},"outputs":[],"execution_count":141},{"cell_type":"markdown","source":"### 2.3 Inference","metadata":{}},{"cell_type":"code","source":"class TinyLlamaChatModel:\n    def __init__(self, model_name: str):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.pipe = pipeline(\n            \"text-generation\", \n            model=model_name, \n            torch_dtype=torch.bfloat16, \n            device_map=\"auto\"\n        )\n\n    def generate_response(self, user_message: str, context: str = \"\", max_new_tokens: int = 256) -> str:\n        messages = []\n        \n        if context: #context addition\n            messages.append({\n                \"role\": \"system\",j\n                \"content\": f\"The following is relevant context for your response. Use this information to help answer the user's question:\\n\\n{context}\"\n            })\n        \n        messages.append({ \n            \"role\": \"user\",\n            \"content\": user_message\n        })\n        \n        prompt = self.tokenizer.apply_chat_template(\n            messages, \n            tokenize=False, \n            add_generation_prompt=True\n        )\n        \n        outputs = self.pipe( #output gen\n            prompt, \n            max_new_tokens=max_new_tokens, \n            do_sample=True, \n            temperature=0.7, \n            top_k=50, \n            top_p=0.95\n        )\n        \n        full_response = outputs[0][\"generated_text\"]\n        assistant_part = full_response.split(\"<|assistant|>\")[-1].strip()\n\n        stop_idx = assistant_part.find(config.stop_word)\n        if stop_idx != -1:\n            assistant_part = assistant_part[:stop_idx].strip()\n            \n        return assistant_part","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T10:48:15.096657Z","iopub.execute_input":"2025-04-01T10:48:15.096979Z","iopub.status.idle":"2025-04-01T10:48:15.103501Z","shell.execute_reply.started":"2025-04-01T10:48:15.096951Z","shell.execute_reply":"2025-04-01T10:48:15.102492Z"}},"outputs":[],"execution_count":150},{"cell_type":"markdown","source":"### 2.4 Bot Functionalities","metadata":{}},{"cell_type":"code","source":"class RAGChatbot:\n    def __init__(self, config: Config):\n        self.config = config\n        self.vector_db = VectorDB(config.vector_dim)\n        self.llm = TinyLlamaChatModel(config.model_name)\n        self.conversation_history = []\n        \n    def chat(self, user_input: str) -> str:\n        if self.config.stop_word in user_input:\n            return \"Chat ended.\" #stopping at trigger word\n             \n        relevant_chunks = self.vector_db.search(user_input, self.config.top_k)\n        context = \"\\n\".join(relevant_chunks)\n        \n        response = self.llm.generate_response( #output gen\n            user_message=user_input,\n            context=context,\n            max_new_tokens=self.config.max_new_tokens\n        )\n        \n        self.conversation_history.append({\"user\": user_input, \"assistant\": response})\n        \n        self.vector_db.add_text(f\"User: {user_input}\\nAssistant: {response}\")\n        \n        return response \n\n    def load_initial_context(self):\n        initial_context = \"\"\"\n        This is example context to be added to the vector database for retrieval. I can add any other relevant textual content to it, to privide context for teh model.\n        So this encourages context rich generation. Huge step eh?!\n        \"\"\"\n        self.vector_db.add_text(initial_context)\n    \n    def display_conversation(self) -> None:\n        for i, exchange in enumerate(self.conversation_history):\n            print(f\"User [{i+1}]: {exchange['user']}\")\n            print(f\"Assistant [{i+1}]: {exchange['assistant']}\")\n            print(\"-\" * 50)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T11:14:33.857015Z","iopub.execute_input":"2025-04-01T11:14:33.857431Z","iopub.status.idle":"2025-04-01T11:14:33.864029Z","shell.execute_reply.started":"2025-04-01T11:14:33.857395Z","shell.execute_reply":"2025-04-01T11:14:33.863144Z"}},"outputs":[],"execution_count":169},{"cell_type":"markdown","source":"## 3. Testing tinyChat","metadata":{}},{"cell_type":"markdown","source":"### 3.1 RUN Function","metadata":{}},{"cell_type":"code","source":"def tinyChat():\n    print(\"TinyLlama RAG Chatbot Initialized!\")\n    print(f\"Type '{config.stop_word}' to end the chat.\")\n    print(\"-\" * 50)\n    \n    while True:\n        user_input = input(\"You: \")\n        if user_input.strip().lower() == config.stop_word.lower():\n            print(\"Chat ended.\")\n            break\n        \n        response = chatbot.chat(user_input)\n        print(f\"Assistant: {response}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T11:19:04.908404Z","iopub.execute_input":"2025-04-01T11:19:04.908707Z","iopub.status.idle":"2025-04-01T11:19:04.913258Z","shell.execute_reply.started":"2025-04-01T11:19:04.908683Z","shell.execute_reply":"2025-04-01T11:19:04.912478Z"}},"outputs":[],"execution_count":180},{"cell_type":"markdown","source":"### 3.2 Initialising tinyChat","metadata":{}},{"cell_type":"code","source":"config = Config()\nchatbot = RAGChatbot(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T11:21:40.925635Z","iopub.execute_input":"2025-04-01T11:21:40.925961Z","iopub.status.idle":"2025-04-01T11:21:43.733937Z","shell.execute_reply.started":"2025-04-01T11:21:40.925933Z","shell.execute_reply":"2025-04-01T11:21:43.733028Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":184},{"cell_type":"markdown","source":"### 3.3 Testing Retrieval Augmented Generation\n#### A fictional story about Ali Goba -- A legendary Indian footballer.\n> We are using this as a context to interact with the chatbot.","metadata":{}},{"cell_type":"code","source":"context = 'The Legend of Ali Goba: From Ladakh to Global Glory \\n In the cold, rugged terrain of Ladakh, where oxygen was thin and dreams often seemed out of reach, a young boy named Ali Goba spent his days kicking a battered football against monastery walls. Born into a humble family in Kargil, Ali had nothing but his raw talent, an unbreakable spirit, and a dream—to play in the biggest stadiums of the world.\\n The Rise from Ladakh \\n Ali’s extraordinary footwork caught the attention of a visiting coach from the AIFF Elite Academy during a youth tournament in Delhi. By the time he was 16, his name was already whispered in Indian footballing circles. With his dazzling dribbles and pinpoint passing, he led the Indian U-17 team to an unexpected triumph at the AFC U-17 Championship, attracting the attention of European scouts. \\n Tottenham Hotspur: The Breakthrough \\n At 18, Ali Goba made history, becoming the first Indian footballer to sign for a Premier League club, joining Tottenham Hotspur. Under the guidance of Mauricio Pochettino, he honed his technical skills and adapted to the lightning-fast pace of the English game. His debut in the North London Derby against Arsenal was nothing short of spectacular—scoring a stunning goal from 30 yards out, making headlines across Europe. \\n By his second season, he had formed a formidable midfield partnership with Christian Eriksen and Dele Alli. His performances against the likes of Manchester City and Liverpool earned him the PFA Young Player of the Year award, marking his arrival as a world-class talent. \\n FC Barcelona: The Making of a Legend \\n His meteoric rise led to a record-breaking €150 million transfer to FC Barcelona, where he donned the legendary number 10 shirt after Lionel Messi’s departure. Playing alongside Pedri and Frenkie de Jong, Ali became the architect of Barcelona’s attack, blending the tiki-taka style with his own Himalayan resilience. \\n It was during El Clásico against Real Madrid that he cemented his place among the greats—scoring a last-minute bicycle kick winner past Thibaut Courtois in front of a roaring Camp Nou. His impact was immediate, leading Barcelona to back-to-back La Liga and Champions League titles. In 2029, he achieved what no Indian had before—winning the Ballon Dor, beating Kylian Mbappé and Jude Bellingham to the prestigious award. \\n Bringing the World Cup to India: \\n Despite his club success, Ali Goba’s heart remained with his homeland. Under his captaincy, India qualified for the 2030 FIFA World Cup—a historic first. Against all odds, India, ranked 72nd in the world, shocked Germany in the quarter-finals, with Ali scoring an outrageous free-kick past Manuel Neuer’s successor. \\n In the final at Maracanã Stadium, Brazil, facing an Argentina side led by Paulo Dybala and Alejandro Garnacho, Ali produced a masterclass. In the dying minutes, he nutmegged Enzo Fernández, dribbled past Lisandro Martínez, and chipped the ball over Emiliano Martínez, securing India’s first-ever World Cup trophy. \\n The Immortal Legacy: \\n Ali Goba returned to India as a national hero, inspiring millions. Stadiums were renamed after him, and football academies sprang up across the country. His autobiography, \"From Ladakh to the World\", became a bestseller, and his story was adapted into a Bollywood blockbuster starring Ranveer Singh. \\n Even after retirement, Ali remained an ambassador for Indian football, mentoring young talents and ensuring that no child in Ladakh—or anywhere in India—ever had to give up on their dreams due to circumstances. \\n To this day, football fans around the world remember the boy from the Himalayas who dared to dream, conquered the world, and changed Indian football forever. \\n Ali Goba’s name stands alongside Pelé, Maradona, Messi, and Ronaldo, proving that legends can rise from anywhere—even the highest mountains of the world.'\nchatbot.vector_db.add_text(context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T11:21:47.012456Z","iopub.execute_input":"2025-04-01T11:21:47.012787Z","iopub.status.idle":"2025-04-01T11:21:47.330823Z","shell.execute_reply.started":"2025-04-01T11:21:47.012759Z","shell.execute_reply":"2025-04-01T11:21:47.329839Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f4fd37070f8468e82f36f6941d8cbfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be77d764b49e4e379e6583b2803b13b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996aaed7734842b79e79bb3f50b00cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91aa5ca10c524b9db0507919b11a6bb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715097e27ed24622947df0dcbff31bd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c420dfcf2eba4de5a3d78d08ec3a8cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e891b8cc80449dcae9df46b9977ff9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aad825049944aabac09431d7725ca59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f08c9088ce9b4276846176296fce8c46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f732fef321b4480a980fbbf9cd84b5b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e662d0f5e5ab4905809adc49ece3c76a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d5d107375ef4b69b6879cc5285eb677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008f45dda6034592942269284fb413e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa048d536094d1d9101cbc6059d0dce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c58d1027ac447a824e6c29ec7b449f"}},"metadata":{}}],"execution_count":185},{"cell_type":"markdown","source":"### 3.4 Chat Window","metadata":{}},{"cell_type":"code","source":"tinyChat()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T11:21:51.116378Z","iopub.execute_input":"2025-04-01T11:21:51.116701Z","iopub.status.idle":"2025-04-01T11:22:38.919902Z","shell.execute_reply.started":"2025-04-01T11:21:51.116673Z","shell.execute_reply":"2025-04-01T11:22:38.919052Z"}},"outputs":[{"name":"stdout","text":"TinyLlama RAG Chatbot Initialized!\nType 'STOP!' to end the chat.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Who was Ali Goba?\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ee940a75d24cabbe2608aac26ec053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b83e6b2d861e41dbab93c90ac89dc606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c0912d165b4678863bcfd0f8f80692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f47e176cff42d58e059292ef105425"}},"metadata":{}},{"name":"stdout","text":"Assistant: Ali Goba is a former Indian football player who is widely regarded as one of the greatest footballers in Indian history. He played for India at the 1974 FIFA World Cup, where he scored the first-ever Indian goal in the tournament. After the tournament, Goba retired from international football and returned to his hometown of Leh, where he continued to play for the local football team, winning several championships in the process. He also served as a coach for the Indian national team during the 1980s and 1990s, helping them to win the Asian Cup in 1996 and the Asian Games in 1998. Goba's impact on Indian football was significant, and he is often considered one of the greatest footballers of all time in India.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Did Ali Goba ever win the ballon dor?\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb6e6d2c6fd49619a4fec24fce47ea2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66cc87d346114d0da59e925992b4b0eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f30e0d961aa43e79f5e41b76a39b9d1"}},"metadata":{}},{"name":"stdout","text":"Assistant: Yes, Ali Goba won the FIFA World Cup Ballon d'Or award in 1974, which is considered to be the most prestigious individual award in football. This award is presented annually to the top goalkeeper or midfielder in the world, and Goba was the first Indian to win it, making him one of the greatest footballers in Indian history.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  STOP!\n"},{"name":"stdout","text":"Chat ended.\n","output_type":"stream"}],"execution_count":186},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}